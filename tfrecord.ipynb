{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp data.tfrecord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import os \n",
    "import os.path as path\n",
    "from typing import Tuple\n",
    "\n",
    "import dateutil.parser as parser\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import skimage\n",
    "import tensorflow as tf\n",
    "from osgeo import gdal\n",
    "\n",
    "import airathon.data as data\n",
    "import airathon.paths as paths\n",
    "from airathon.model.modis import load_modis\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "Dataset = tf.data.Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.7.0'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF Record IO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export \n",
    "grid_metadata = pd.read_csv(path.join(\n",
    "    paths.dataset_metadata(), \"grid_metadata.csv\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def get_grid_data(metadata: pd.DataFrame, grid_id: str) -> pd.DataFrame:\n",
    "    return metadata[metadata[\"grid_id\"] == grid_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export \n",
    "satellite_metadata = pd.read_csv(path.join(\n",
    "    paths.dataset_metadata(), \"satellite_metadata.csv\"))\n",
    "\n",
    "satellite_metadata['Date'] = pd.to_datetime(\n",
    "    satellite_metadata['time_end'], format='%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tpe', 'dl', 'la'}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>granule_id</th>\n",
       "      <th>time_start</th>\n",
       "      <th>time_end</th>\n",
       "      <th>product</th>\n",
       "      <th>location</th>\n",
       "      <th>split</th>\n",
       "      <th>us_url</th>\n",
       "      <th>eu_url</th>\n",
       "      <th>as_url</th>\n",
       "      <th>cksum</th>\n",
       "      <th>granule_size</th>\n",
       "      <th>Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20180201T191000_maiac_la_0.hdf</td>\n",
       "      <td>2018-02-01T17:25:00.000Z</td>\n",
       "      <td>2018-02-01 19:10:00+00:00</td>\n",
       "      <td>maiac</td>\n",
       "      <td>la</td>\n",
       "      <td>train</td>\n",
       "      <td>s3://drivendata-competition-airathon-public-us...</td>\n",
       "      <td>s3://drivendata-competition-airathon-public-eu...</td>\n",
       "      <td>s3://drivendata-competition-airathon-public-as...</td>\n",
       "      <td>911405771</td>\n",
       "      <td>10446736</td>\n",
       "      <td>2018-02-01 19:10:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20180202T195000_maiac_la_0.hdf</td>\n",
       "      <td>2018-02-02T18:05:00.000Z</td>\n",
       "      <td>2018-02-02 19:50:00+00:00</td>\n",
       "      <td>maiac</td>\n",
       "      <td>la</td>\n",
       "      <td>train</td>\n",
       "      <td>s3://drivendata-competition-airathon-public-us...</td>\n",
       "      <td>s3://drivendata-competition-airathon-public-eu...</td>\n",
       "      <td>s3://drivendata-competition-airathon-public-as...</td>\n",
       "      <td>2244451908</td>\n",
       "      <td>11090180</td>\n",
       "      <td>2018-02-02 19:50:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20180203T203000_maiac_la_0.hdf</td>\n",
       "      <td>2018-02-03T17:10:00.000Z</td>\n",
       "      <td>2018-02-03 20:30:00+00:00</td>\n",
       "      <td>maiac</td>\n",
       "      <td>la</td>\n",
       "      <td>train</td>\n",
       "      <td>s3://drivendata-competition-airathon-public-us...</td>\n",
       "      <td>s3://drivendata-competition-airathon-public-eu...</td>\n",
       "      <td>s3://drivendata-competition-airathon-public-as...</td>\n",
       "      <td>3799527997</td>\n",
       "      <td>12468482</td>\n",
       "      <td>2018-02-03 20:30:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20180204T194000_maiac_la_0.hdf</td>\n",
       "      <td>2018-02-04T17:55:00.000Z</td>\n",
       "      <td>2018-02-04 19:40:00+00:00</td>\n",
       "      <td>maiac</td>\n",
       "      <td>la</td>\n",
       "      <td>train</td>\n",
       "      <td>s3://drivendata-competition-airathon-public-us...</td>\n",
       "      <td>s3://drivendata-competition-airathon-public-eu...</td>\n",
       "      <td>s3://drivendata-competition-airathon-public-as...</td>\n",
       "      <td>4105997844</td>\n",
       "      <td>13064424</td>\n",
       "      <td>2018-02-04 19:40:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20180205T202000_maiac_la_0.hdf</td>\n",
       "      <td>2018-02-05T17:00:00.000Z</td>\n",
       "      <td>2018-02-05 20:20:00+00:00</td>\n",
       "      <td>maiac</td>\n",
       "      <td>la</td>\n",
       "      <td>train</td>\n",
       "      <td>s3://drivendata-competition-airathon-public-us...</td>\n",
       "      <td>s3://drivendata-competition-airathon-public-eu...</td>\n",
       "      <td>s3://drivendata-competition-airathon-public-as...</td>\n",
       "      <td>1805072340</td>\n",
       "      <td>12549313</td>\n",
       "      <td>2018-02-05 20:20:00+00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       granule_id                time_start  \\\n",
       "0  20180201T191000_maiac_la_0.hdf  2018-02-01T17:25:00.000Z   \n",
       "1  20180202T195000_maiac_la_0.hdf  2018-02-02T18:05:00.000Z   \n",
       "2  20180203T203000_maiac_la_0.hdf  2018-02-03T17:10:00.000Z   \n",
       "3  20180204T194000_maiac_la_0.hdf  2018-02-04T17:55:00.000Z   \n",
       "4  20180205T202000_maiac_la_0.hdf  2018-02-05T17:00:00.000Z   \n",
       "\n",
       "                    time_end product location  split  \\\n",
       "0  2018-02-01 19:10:00+00:00   maiac       la  train   \n",
       "1  2018-02-02 19:50:00+00:00   maiac       la  train   \n",
       "2  2018-02-03 20:30:00+00:00   maiac       la  train   \n",
       "3  2018-02-04 19:40:00+00:00   maiac       la  train   \n",
       "4  2018-02-05 20:20:00+00:00   maiac       la  train   \n",
       "\n",
       "                                              us_url  \\\n",
       "0  s3://drivendata-competition-airathon-public-us...   \n",
       "1  s3://drivendata-competition-airathon-public-us...   \n",
       "2  s3://drivendata-competition-airathon-public-us...   \n",
       "3  s3://drivendata-competition-airathon-public-us...   \n",
       "4  s3://drivendata-competition-airathon-public-us...   \n",
       "\n",
       "                                              eu_url  \\\n",
       "0  s3://drivendata-competition-airathon-public-eu...   \n",
       "1  s3://drivendata-competition-airathon-public-eu...   \n",
       "2  s3://drivendata-competition-airathon-public-eu...   \n",
       "3  s3://drivendata-competition-airathon-public-eu...   \n",
       "4  s3://drivendata-competition-airathon-public-eu...   \n",
       "\n",
       "                                              as_url       cksum  \\\n",
       "0  s3://drivendata-competition-airathon-public-as...   911405771   \n",
       "1  s3://drivendata-competition-airathon-public-as...  2244451908   \n",
       "2  s3://drivendata-competition-airathon-public-as...  3799527997   \n",
       "3  s3://drivendata-competition-airathon-public-as...  4105997844   \n",
       "4  s3://drivendata-competition-airathon-public-as...  1805072340   \n",
       "\n",
       "   granule_size                      Date  \n",
       "0      10446736 2018-02-01 19:10:00+00:00  \n",
       "1      11090180 2018-02-02 19:50:00+00:00  \n",
       "2      12468482 2018-02-03 20:30:00+00:00  \n",
       "3      13064424 2018-02-04 19:40:00+00:00  \n",
       "4      12549313 2018-02-05 20:20:00+00:00  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size = 7721\n"
     ]
    }
   ],
   "source": [
    "print(set(satellite_metadata.loc[:, \"location\"]))\n",
    "display(satellite_metadata.head())\n",
    "print(f\"size = {len(satellite_metadata)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def get_satellite_meta(\n",
    "        metadata, datetime: str, location: str, datatype: str, split: str):\n",
    "    if location == \"Delhi\":\n",
    "        location = \"dl\"\n",
    "    elif location == \"Taipei\":\n",
    "        location = \"tpe\"\n",
    "    else:\n",
    "        location = \"la\"\n",
    "\n",
    "    # filtering\n",
    "    metadata = metadata[metadata['location'] == location]\n",
    "    metadata = metadata[metadata['product'] == datatype]\n",
    "    metadata = metadata[metadata['split'] == split]\n",
    "    dateobject = parser.parse(datetime)\n",
    "\n",
    "    return metadata.loc[(metadata['Date'].dt.month == dateobject.month) &\n",
    "                        (metadata['Date'].dt.day == dateobject.day) &\n",
    "                        (metadata['Date'].dt.year <= dateobject.year)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "maiac_subset_names = [f\"sds_{i}\" for i in range(0, 13)]\n",
    "maiac_subset_indices = [0, 3, 4, 8]\n",
    "\n",
    "def fetch_subset(year: str, granule_id: str, split: str) -> dict:\n",
    "    modis = load_modis(year, granule_id, split)\n",
    "    subdataset = modis.GetSubDatasets()  # List[tuple]\n",
    "\n",
    "    features = dict()\n",
    "    rasters = list()\n",
    "\n",
    "    for index in maiac_subset_indices:\n",
    "        url, _ = subdataset[index]\n",
    "        raster = gdal.Open(url)\n",
    "        raster = raster.ReadAsArray()\n",
    "\n",
    "        raster = np.swapaxes(raster, 0, 2)\n",
    "        rasters.append(raster)\n",
    "\n",
    "        raster = skimage.transform.resize(\n",
    "            raster, \n",
    "            output_shape=(240, 240, 4), \n",
    "            anti_aliasing=False)\n",
    "        \n",
    "        features[maiac_subset_names[index]] = raster.astype(np.float32)\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['sds_0', 'sds_3', 'sds_4', 'sds_8'])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset_features = fetch_subset(\n",
    "    year=\"2018\",\n",
    "    granule_id=\"20180201T191000_maiac_la_0.hdf\", \n",
    "    split=\"train\")\n",
    "\n",
    "subset_features.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>grid_id</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-02-01T08:00:00Z</td>\n",
       "      <td>3S31A</td>\n",
       "      <td>11.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-02-01T08:00:00Z</td>\n",
       "      <td>A2FBI</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-02-01T08:00:00Z</td>\n",
       "      <td>DJN0F</td>\n",
       "      <td>11.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-02-01T08:00:00Z</td>\n",
       "      <td>E5P9N</td>\n",
       "      <td>22.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-02-01T08:00:00Z</td>\n",
       "      <td>FRITQ</td>\n",
       "      <td>29.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               datetime grid_id  value\n",
       "0  2018-02-01T08:00:00Z   3S31A   11.4\n",
       "1  2018-02-01T08:00:00Z   A2FBI   17.0\n",
       "2  2018-02-01T08:00:00Z   DJN0F   11.1\n",
       "3  2018-02-01T08:00:00Z   E5P9N   22.1\n",
       "4  2018-02-01T08:00:00Z   FRITQ   29.8"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\n",
    "    path.join(paths.dataset_metadata(), \"train_labels.csv\"))\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating TF Record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def image_feature(image: np.ndarray):\n",
    "    image = image.flatten()\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=image))\n",
    "\n",
    "\n",
    "def float_feature(v: float):\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=[v]))\n",
    "\n",
    "\n",
    "def string_feature(s: str):\n",
    "    bs = bytes(s, \"UTF-8\")\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[bs]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def label_to_example(label: dict) -> tf.train.Example:\n",
    "    feature = {\n",
    "        \"location\": string_feature(label[\"location\"]),\n",
    "        \"grid_id\": string_feature(label[\"grid_id\"]),\n",
    "        \"datetime\": string_feature(label[\"datetime\"]),\n",
    "        \"value\": float_feature(label[\"value\"])\n",
    "    }\n",
    "\n",
    "    del label[\"location\"]\n",
    "    del label[\"grid_id\"]\n",
    "    del label[\"datetime\"]\n",
    "    del label[\"value\"]\n",
    "\n",
    "    for name, image in label.items():\n",
    "        feature[name] = image_feature(image) \n",
    "\n",
    "    example = tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "    \n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def series_to_locations(series):\n",
    "    datetime = series[\"datetime\"]  # type: str\n",
    "    grid_id = series[\"grid_id\"]\n",
    "    grid_data = get_grid_data(grid_metadata, grid_id)\n",
    "    location = grid_data.iloc[0][\"location\"]\n",
    "\n",
    "    if location == \"Delhi\":\n",
    "        location = \"dl\"\n",
    "    elif location == \"Taipei\":\n",
    "        location = \"tpe\"\n",
    "    else:\n",
    "        location = \"la\"\n",
    "\n",
    "    return location\n",
    "\n",
    "\n",
    "def series_to_subset_infos(series, split = \"train\"):\n",
    "    datetime = series[\"datetime\"]  # type: str\n",
    "    grid_id = series[\"grid_id\"]\n",
    "    location = series[\"location\"]\n",
    "\n",
    "    satellite_data = get_satellite_meta(\n",
    "        satellite_metadata,\n",
    "        datetime,\n",
    "        location,\n",
    "        \"maiac\",  # or 'misr'\n",
    "        split)\n",
    "\n",
    "    infos = list()\n",
    "\n",
    "    for i in range(len(satellite_data)):\n",
    "        granule_id = satellite_data.iloc[i]['granule_id']\n",
    "        time_end = parser.parse(satellite_data.iloc[i][\"time_end\"])\n",
    "\n",
    "        infos.append((granule_id, time_end))\n",
    "\n",
    "    return infos \n",
    "\n",
    "def row_to_label(row, split):\n",
    "    datetime = row[\"datetime\"]  # type: str\n",
    "    grid_id = row[\"grid_id\"]\n",
    "    location = row[\"location\"]\n",
    "\n",
    "    subset_infos = row[\"subset_info\"]\n",
    "\n",
    "    images = {}\n",
    "\n",
    "    for index in maiac_subset_indices:\n",
    "        name = maiac_subset_names[index]\n",
    "        images[name] = list()\n",
    "\n",
    "    for granule_id, time_end in subset_infos:\n",
    "        new_images = fetch_subset(str(time_end.year), granule_id, split)\n",
    "\n",
    "        for name, image in new_images.items():\n",
    "            images[name].append(image)\n",
    "\n",
    "    for index in maiac_subset_indices:\n",
    "        name = maiac_subset_names[index]\n",
    "        images_name = np.array(images[name])\n",
    "        images[name] = images_name.mean(axis=0)\n",
    "\n",
    "    label = {\n",
    "        \"location\": location,\n",
    "        \"grid_id\": grid_id,\n",
    "        \"datetime\": datetime,\n",
    "        \"value\": row[\"value\"],\n",
    "        **images\n",
    "    } \n",
    "\n",
    "    return label \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['location', 'grid_id', 'datetime', 'value', 'sds_0', 'sds_3', 'sds_4', 'sds_8'])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = df.sample(n=10, axis=0, random_state=0)\n",
    "test_df[\"location\"] = test_df.apply(series_to_locations, axis=1)\n",
    "test_df[\"subset_info\"] = test_df.apply(series_to_subset_infos, axis=1, split=\"train\")\n",
    "\n",
    "label = row_to_label(test_df.iloc[0], \"train\")\n",
    "label.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def create_tfrecord(dataframe: pd.DataFrame, split: str, path: str):\n",
    "    dataframe = dataframe.copy()\n",
    "\n",
    "    print(\"fetching locations...\")\n",
    "    dataframe[\"location\"] = dataframe.apply(series_to_locations, axis=1)\n",
    "\n",
    "    print(\"fetching subset_infos...\")\n",
    "    dataframe[\"subset_info\"] = dataframe.apply(series_to_subset_infos, axis=1, split=split)\n",
    "\n",
    "    with tf.io.TFRecordWriter(path) as writer, tqdm(total=len(dataframe)) as progress_bar:\n",
    "        for _, row in dataframe.iterrows():\n",
    "            label = row_to_label(row, split)\n",
    "            example = label_to_example(label)\n",
    "\n",
    "            writer.write(example.SerializeToString())\n",
    "\n",
    "            progress_bar.update(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fetching locations...\n",
      "fetching subset_infos...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:02<00:00,  1.19it/s]\n"
     ]
    }
   ],
   "source": [
    "create_tfrecord(df.iloc[0:3], \"train\", \"test.tfrecord\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading TF Record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def _decode(raw_person):\n",
    "    spec = {\n",
    "        \"value\": tf.io.FixedLenFeature([], dtype=tf.float32),\n",
    "        \"location\": tf.io.FixedLenFeature([], dtype=tf.string),\n",
    "        \"datetime\": tf.io.FixedLenFeature([], dtype=tf.string),\n",
    "        \"grid_id\": tf.io.FixedLenFeature([], dtype=tf.string),\n",
    "        # \"sds_0\": tf.io.FixedLenFeature((240, 240, 4), dtype=tf.float32),\n",
    "        # \"sds_1\": tf.io.FixedLenFeature((240, 240, 4), dtype=tf.float32),\n",
    "        # \"sds_2\": tf.io.FixedLenFeature((240, 240, 4), dtype=tf.float32),\n",
    "        # \"sds_3\": tf.io.FixedLenFeature((240, 240, 4), dtype=tf.float32),\n",
    "        # \"sds_4\": tf.io.FixedLenFeature((240, 240, 4), dtype=tf.float32),\n",
    "        # \"sds_5\": tf.io.FixedLenFeature((240, 240, 4), dtype=tf.float32),\n",
    "        # \"sds_6\": tf.io.FixedLenFeature((240, 240, 4), dtype=tf.float32),\n",
    "        # \"sds_7\": tf.io.FixedLenFeature((240, 240, 4), dtype=tf.float32),\n",
    "        # \"sds_8\": tf.io.FixedLenFeature((240, 240, 4), dtype=tf.float32),\n",
    "        # \"sds_9\": tf.io.FixedLenFeature((240, 240, 4), dtype=tf.float32),\n",
    "        # \"sds_10\": tf.io.FixedLenFeature((240, 240, 4), dtype=tf.float32),\n",
    "        # \"sds_11\": tf.io.FixedLenFeature((240, 240, 4), dtype=tf.float32),\n",
    "        # \"sds_12\": tf.io.FixedLenFeature((240, 240, 4), dtype=tf.float32),\n",
    "    }\n",
    "\n",
    "    for index in maiac_subset_indices:\n",
    "        name = maiac_subset_names[index]\n",
    "        spec[name] = tf.io.FixedLenFeature((240, 240, 4), dtype=tf.float32)\n",
    "\n",
    "    return tf.io.parse_single_example(raw_person, spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def load_tfrecord(path: str):\n",
    "    return tf.data.TFRecordDataset(path).map(_decode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'datetime': TensorSpec(shape=(), dtype=tf.string, name=None),\n",
       " 'grid_id': TensorSpec(shape=(), dtype=tf.string, name=None),\n",
       " 'location': TensorSpec(shape=(), dtype=tf.string, name=None),\n",
       " 'sds_0': TensorSpec(shape=(240, 240, 4), dtype=tf.float32, name=None),\n",
       " 'sds_3': TensorSpec(shape=(240, 240, 4), dtype=tf.float32, name=None),\n",
       " 'sds_4': TensorSpec(shape=(240, 240, 4), dtype=tf.float32, name=None),\n",
       " 'sds_8': TensorSpec(shape=(240, 240, 4), dtype=tf.float32, name=None),\n",
       " 'value': TensorSpec(shape=(), dtype=tf.float32, name=None)}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "read_ds = load_tfrecord(\"test.tfrecord\")\n",
    "read_ds.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['datetime', 'grid_id', 'location', 'sds_0', 'sds_3', 'sds_4', 'sds_8', 'value'])\n",
      "(240, 240, 4)\n",
      "\n",
      "dict_keys(['datetime', 'grid_id', 'location', 'sds_0', 'sds_3', 'sds_4', 'sds_8', 'value'])\n",
      "(240, 240, 4)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for element in read_ds.take(2):\n",
    "    print(element.keys())\n",
    "    print(element[\"sds_0\"].shape)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.10 ('competition')",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
